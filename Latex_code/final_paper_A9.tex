\documentclass[11pt,sigconf]{acmart}
\usepackage{tikz}

\usetikzlibrary{positioning}
\begin{document}

\title{Group-3}

\author{Saba Sultana}
\email{s.fnu5@student.fdu.edu}
\affiliation{%
  \institution{Fairleigh Dickinson University}
  \city{Vancouver}
  \state{British Columbia}
  \country{Canada}
}
\author{Prince Roy}
\email{p.roy@student.fdu.edu}
\affiliation{%
  \institution{Fairleigh Dickinson University}
  \city{Vancouver}
  \state{British Columbia}
  \country{Canada}
}
\author{Xu Zhao}
\email{x.zhao2@student.fdu.edu}
\affiliation{%
  \institution{Fairleigh Dickinson University}
  \city{Vancouver}
  \state{British Columbia}
  \country{Canada}
}
\author{Anastasia Kravchenko}
\email{anastasiia@student.fdu.edu}
\affiliation{%
  \institution{Fairleigh Dickinson University}
  \city{Vancouver}
  \state{British Columbia}
  \country{Canada}}
\author{Ayush Kharel}
\email{a.kharel@student.fdu.edu}
\affiliation{%
  \institution{Fairleigh Dickinson University}
  \city{Vancouver}
  \state{British Columbia}
  \country{Canada}}
  \maketitle

\section{Abstract}
In hybrid storage systems that employ the use of SSDs and hard drives, the cache management policy in SSDs can't be overemphasized. These cache management policies should ensure that the storage system has quick response times as well as ensure the longevity of the SSD. The usual approach, like in LRU, has a significant issue since it foolishly assumes that the time it takes to access the hard drive, which takes longer, takes the same time as when it accesses the SSD.

Therefore, to address this challenge, three cache management policiestested in a simulator were LRU, DT-SLRU, and EDE. The impact of the three was determined based on the usual server workload.The results obtained show that cache management policiesthat understand the 'high cost' involved in seeking 'data from the hard disk make the storage approach much more stable, as it helps us use our resources in a more efficient manner, in particular, the values obtained when using the EDE policy show that it gives a PeaksDisk Head Time of 4310 ms, along with a stable median service time around 53 ms, while improving cache memory usage by roughly 55\%. The implications here are clear: the choice of cache management policy has a massive impact in determining the optimal use of the hybrid cache storage. The cache management policiestthat work well are those that understand the slowness associated with the storage in the hard disk, along with the dynamic behavior exhibited by workloads in the storage system.

\clearpage
\newpage\section{Introduction}
Modern storage solutions combine fast Solid-State Drives (SSDs) with Hard Disk Drives (HDD) to optimize hybrid storage architectures, which take advantage of these two contrasting technologies {38}. The idea is to exploit the big I/O-bandwidth and low access delays of SSD storage to store frequently accessed information, while taking advantage of HDD storage capacities and corresponding low costs per gigabyte. A very important issue that emerges with this kind of hybrid design is how to distribute these information blocks within the limited SSD memory to optimize overall system performance and avoid expensive access delays.
The conventional caching algorithm, Least Recently Used (LRU), is based on a very simple principle: remove the item which has been accessed least recently. Though very simple and successful on single-tier caching, this algorithm holds a misconception about its applicability to a hybrid caching scenario—it considers each cache miss to be identical.Accessing an SSD is significantly faster compared to an HDD. As such, one can assert that LRU is oblivious to such varying access costs and could result in suboptimal caching decisions. This could result in caching transient and less valuable items within the very limited SSD cache while removing less transient but vital items, which could lead to increased accesses to the HDD, causing poor system performance during peak usage periods {5}.
This has led to the design of “cost-aware” caching algorithms specifically geared toward hybrid applications. As an example, DT-SLRU (Disk-Time Segmented LRU) sizes up each data object on a “cost score” reflecting how long it will take to access it via HDD accesses {55}. This allows more expensive-to-access objects to be preferentially preserved within the SSD buffer. Another state-of-the-art algorithm is EDE (Episode-Deadline Eviction), which defines “protected zones” within each cache to preserve critical information like file system metadata against early eviction during extreme I/O intensive episodes {1}. The purpose of our research is to fill this gap by carrying out a comprehensive empirical study to evaluate these caching strategies. We systematically compare LRU, DT-SLRU, and EDE on the same trace while varying key parameters. The emphasis is on how these modifications affect key metrics such as Peak Disk-head Time (worst-case response time), Median Disk-head Time (average response time), and system variability. We go beyond merely finding which algorithm is best and instead explore how these modifications affect these metrics and analyze how and why these modifications occur. For instance, we analyze how raising the threshold value in DT can decrease peak response times up until a saturation level is reached to avoid increased overhead, and how Protected Cap and Predictor Responsiveness (TTI) display U-shaped characteristics.

Our principal contributions are fourfold:
We present a comparative analysis of three eviction policies. We compare and contrast LRU, DT-SLRU, and EDE on an equal footing to spell out their characteristics within a setting where the access rate differences between SSD and HDD are extreme {\href{}{1}},. This will serve us well to compare their efficacies.
To find out the best setting for these parameters, we conduct an ablation study. Through varying each parameter one at a time (DT, PC, TTI), we explore how these parameters interact to affect system behavior. This helps us to discover system configurations that can properly respond to workload dynamics without being sensitive to transient spikes, thus finding a balance between responsiveness and stability {1, 55}.
We show the effectiveness of our ‘cost-aware’ method. The outcomes confirm that policies informed about the high expense associated with misses on disks make better use of limited flash resources. They outperform LRU by a considerable margin to deal with both tail and cache accesses, which provides a strong basis to build future memory cache solutions upon {55}.
This paper gives a roadmap on how to build self-optimizing (autotuning) systems. An important observation arising out of our work is that to make caching self-optimize successfully, there is a need to understand what these variables signify and how sensitive they are to each other {1}. 
\subsection{Contribution}
{1} (Baleen): Justifies the evaluation methodology and provides context for comparing different caching strategies under consistent conditions using trace-driven analysis.
{2} (CacheLib): Supports the technical foundation for flash cache evaluation and provides baseline comparison points for traditional strategies like LRU.
{3} (CacheSack): Provides evidence that parameter optimization in cost-aware policies (like DT-SLRU) is crucial for achieving stable system performance.

	


\clearpage
\newpage
\section{Background}
In our study we evaluated the hybrid SSD–HDD caching possibilities and processes. Nowadays most of the modern large-scale storage systems in their majority rely on flash caches to avoid and minimize expensive disk I/O. It also helps to prevent backend disks from becoming overloaded during peak demand periods. HDDs seem to have mechanical delays during every seek operation. We chose to consider the Disk-head Time as the primary factor influencing the system’s latency profile. When workloads caused increases in large or random reads, DT—which represented the total time taken to search for and read blocks from disk—escalated rapidly. Peak DT refers to the intense spikes produced by the alignment of these bursts. Since Peak DT indicated the required number of HDDs to sustain performance during peak load times, it served as the primary measure of backend stress in our analysis. Consequently, reducing Peak DT was crucial for improving system stability and decreasing operational costs.
We examined the movement of objects through flash during execution to evaluate caching behavior. Although a mishandle by the flash cache was nearly free compared to a miss that went to disk, conventional systems like LRU regarded every miss the same. Due to LRU's tendency to retain small or inexpensive items while discarding larger objects that could lead to expensive disk accesses, our investigation revealed that this discrepancy often led to inefficient flash usage. To resolve this discrepancy, advanced strategies such as DT-SLRU and EDE were utilized. DT-SLRU minimized pollution from transient accesses by elevating an object only after it had gathered adequate "disk-time credit." EDE stored valuable items in a secured zone, also predicting their future use by observing their actions during an event. While we analyzed these approaches, we found that both tactics revealed a broader concept that guided our study: effective flash caching required grasping the cost associated with each miss, not merely the total number of misses.
We developed this concept further by examining the occurrence of temporal locality in the traces provided in the Baleen artifact, as our research focused on reproducing results from the Baleen publication. Access distributions with long tails, where a small number of items represented a large share of the disk-time cost, were observed in different workloads. While these objects didn't always occur the most frequently, their absence led to expensive disk scans. This behavior reinforced one of Baleen's goals: to create a caching method that prioritized items based on predicted future DT impact instead of frequency-driven heuristics. Through the analysis of these patterns, we found that the needs of DT-sensitive systems couldn't be entirely satisfied by a design that relied solely on frequency or recency guidance.
We employed an episode-based framework to improve our analysis. The inherent clustering of accesses observed in real workload traces was reflected in an episode, which specified the uninterrupted duration an object remained in cache following its admission. By examining episodes rather than individual accesses, we were able to evaluate the total DT saved across multiple interconnected accesses. The OPT oracle required this perspective to become mandatory for the system. The OPT system evaluated episodes through flash write expenses against DT cost savings while operating independently with complete knowledge of future events. The system selected its most valuable episodes until it used up all available budget. The supervised training pipeline of Baleen received labels from OPT for its online admission model even though OPT operated as a theoretical system. Our research depended on OPT to monitor the ML model because we needed it to select items that would produce major reductions in future DT spikes.
The evaluation process revealed that episode abstraction helped us understand the natural occurrence of prefetching opportunities in trace data. The system detected upcoming user behavior through its ability to detect both consecutive file requests and consistent time intervals between requests. The machine learning algorithm demonstrated strong confidence when it detected these specific patterns. The system demonstrated how prefetching operations interacted with admission mechanisms to decrease unexpected disk requests which resulted in lower Peak DT during busy periods.
The admission model of Baleen used trained attributes to determine flash entry eligibility for objects based on their size and recency and sequential access patterns and interarrival time patterns. The model achieved success in identifying objects which would create major backend problems through its learning process of OPT's cost-based cost optimization. The system operated through different mechanisms than heuristics because it focused on different performance metrics. Our experimental results showed that DT-awareness-based decision making produced better tail latency results but hit rate optimization did not automatically lead to reduced backend stress.
Our team developed an ML-based prefetching system which operated as part of our research. The system used prefetching to predict upcoming requests by storing relevant data in flash storage before system requirements demand it. A properly timed prefetch operation helps reduce DT and creates a smoother load during peak hours in hybrid storage systems by blocking disk access from entering the critical path. Baleen only performed prefetch operations when its predictions reached a threshold level of confidence to prevent unnecessary flash writes that would reduce SSD lifespan. Our research showed that prefetching and admission work together because prefetching stops cold misses which cause DT spikes while admission ensures prefetched blocks remain valuable.
The Baleen-TCO framework served as our solution to handle the expenses associated with flash storage degradation. The system's SSD lifespan shortens when admission strategies become too aggressive because each flash operation reduces the device's available write cycles. The implementation of strict write-rate limits leads to increased DT spikes which forces users to purchase additional HDD storage. The system used multiple write-rate configurations to find the optimal setting which minimized SSD wear expenses and HDD acquisition costs through Baleen-TCO. Our research demonstrated that no single write rate proved optimal for all situations because the system required performance-endurance tradeoffs instead of maximizing one aspect.
We use the two system-architectural diagrams to support this background section depicting the data path operations from our experimental tests. The first diagram shows the complete path of an incoming request as it passed through the flash cache before potentially returning to the HDD.

\begin{figure}[h]
\centering
\begin{tikzpicture}
    box/.style={rectangle,draw,minimum width=2.8cm, minimum height=0.9cm},
    arrow/.style={->,thick}
    ]
\node[box](req){Incoming request};
\node[box,right=1.5cm of req](cache){Flash Cache};
\node[box,right=1.0cm of cache](hdd){HDD Backend}

\draw[arrow](req)--(cache);
\draw[arrow](cache) --node[above]{Miss} (hdd);
\draw[arrow](cache.west)--++(-2,0)node[midway,above]{Hit};
\end{tikzpicture}

\caption{Flash Cache access flow }
\label{fig:flash-cache-flow}
\end{figure}
The second diagram shows how the ML-guided admission and prefetching components interacted to determine flash residency decisions during evaluation.

\begin{figure}[h]
\centering
\begin{tikzpicture}[
    box/.style={rectangle,draw,minimum width=1.0cm, minimum height=0.9cm},
    arrow/.style={->,thick}
    ]
\node[box](features){Episode Features};
\node[box,right=1.0cm of req](mlmodel){ML Admission Model};
\node[box,right=1.0cm of cache](flash){Flash Cache};
\node [box,below=1.0cm of mlmodel](prefetch){Prefetch Decision};

\draw[arrow](features)--(mlmodel);
\draw[arrow](mlmodel)--node[above,yshift=10pt]{\;\;Admit?\;\;\;}(flash);
\draw[arrow](mlmodel)--(prefetch);
\draw[arrow](prefetch)--node[above]{prefetch}(flash);
\end{tikzpicture}
\caption{ML-guided admission and prefetching interaction}
\label{fig:Ml-admission-prefetch}
\end{figure}
The expanded framework enabled our team to develop the essential theoretical foundation which explained both methods and results. Our research built its foundation on a detailed explanation of storage technology systems through our analysis of hybrid storage behavior and episode-driven reasoning and OPT supervision and machine-learning-based admission and prefetching and cost-aware write-rate selection. The continuous viewpoint enabled us to connect Baleen's design to our evaluation which focused on Peak DT reduction and system stability improvement and practical storage efficiency assessment.
\clearpage
\newpage
\newpage
\section{Related Work}
The evolution of flash caching system design and optimization in the past five years has been strongly influenced by the requirement for modern storage systems to manage a trade-off among Endurance, Cost and Performance. In this section, we briefly review the major advancements in prefetching and eviction polices, ML-driven caching systems as well as flash cache admission policies with their pros and cons, and research gaps.

Admission policies to the flash-cache are necessary for managing the balance between flash-life and performance. To constrain the write rate to flash and extend its lifetime, the traditional infectious post-filter method employs the probabilistic or frequency-based filtering. Although such methods are simple and effective, however they lack the flexibility to adapt to workload changes so that their performance tends to degrade in different situations. More flexible, modular admission policies (and random and history-based strategies in particular) were implemented by Meta’s open-source caching engine CacheLib. However, these strategies are still based on applying static threshold values, which can limit their applicability in the presence of highly unpredictable workloads. The approach of Google’s CacheSack is different, since it focuses on maximizing admission using a fractional knapsack. It effectively condenses the places of flash writes by classifying simple admission policies with probability. Despite these benefits, the offline nature of CacheSack can lead to responsiveness issues with respect to changing access patterns. Another closely related work, TinyLFU, is designed to be space efficient and leverages frequency-based boosting. However, being more focused on hit count than other end-to-end performance metrics, it is not what we were looking for in case latency and throughput start to play an essential role. These works collectively draw attention to the need for admission policies that can continuously trade-off write endurance vs hit density vs. backend load reduction in large-scale storage systems whose performance requirements are time-variable.

Machine learning on caching has also seen development of more intelligent, dynamic decision making towards increasingly diverse optimization choices. For example, Flashield registers used counts and their corresponding pages in its DRAM buffer and passes them to SVM classifier for admission. While promising, this approach is limited due to the short lifetime of data in DRAM (i.e., even short-lived features that can fit into memory are not usable), as well as low-quality free training data. Focusing on the fraction of hits, RL-Cache uses reinforcement learning for caching in a CDN. It does not factor in flash-specific restrictions, though—like write endurance—which are essential for storage that contains flash caching. In spite of being powerful and commonly used, Meta's production machine learning policy optimizes intermediate metrics rather than lower-level objectives such as reducing peak backend load. Baleen circumvents these limitations by decomposing caching into three core components: eviction, prefetching and admission. To reduce disk-head time (which significantly affects system performance) it introduces an episode-based residency model to train machine learning policies. Unlike previous work that adopts fixed prefetching or admission strategies, Baleen co-designs them and achieves 12percent reduction in the peak backend load compared to baselines. The use of GBMs make the model to be production-friendly as it ensures both computational efficiency and model interpretability. Despite more complex variants (i.e., the Cache Transformer) previously studied, none of them have been as successful to be GBM due to the likes of class imbalance and no direct signals towards prefetching decisions. The success of Baleen highlights the importance of aligning end-to-end performance goals (and not intermediary ones) with ML-based caching solutions.

The eviction and prefetching mechanisms also directly affect the lifetime of flash memory and data access; thus, both of them are important for flash caching systems. Prefetch can reduce such compulsory misses by anticipating the future data access. Yet, it also introduces the danger of overfetching, worsening the performance and speeding flash wearing. The problem with LruCache is that it does not handle cache misses effectively, so joining them together, we have the CacheSack! The difference between Leaper and CacheSack is that Leaper pre-fetches key ranges in LSM-trees using ML and CacheSack uses static pre-fetching as a part of its optimization framework. However, the static only confidence estimations in their methods lead to suboptimum trade-offs between write amplification and hit rate. To solve this, Baleen proposes two models, namely and, to estimate the best prefetch range and confidence level in the benefits of prefetching.
\clearpage
\newpage
\section{Methodology}
This article compares the behaviour of different caching methods in a hybrid storage system formed by magnetic disks and flash memory. To obtain realistic estimates of the performance of two-tier storage hierarchies commonly employed in enterprise environments, we use a trace-driven simulation environment modeled after the Baleen cache simulator. Though the cache management issue is complex, it can be studied in depth using the simulator by managing a smaller flash cache in front of a larger magnetic disk at block level.

The primary purpose of this approach is to establish a reproducible, standardized environment that can be used  to compare different eviction strategies. For a fair comparison, the simulation replays the same I/O traces for all policy settings. This technique ensures that performance differences are due only to the cache management mechanism and filters out fluctuation factors of workloads. For the same purpose, we begin each experimental repetition with cache being in a cold state and give until initial steady state is established to provide enlightening comparisons on latency effects and hit rates.

The classic Least Recently Used (LRU) is also a benchmarked cache eviction policy used for contrast. Due to its simplicity and low cost, LRU is a practical baseline in many hybrid storage systems. LRU-Refer and Disk-Time Segmented LRU Motivated by the observation that longer disk seek times should be considered more important to retain than shorter ones, the first, Disk-Time Segmented LRU (DT-SLRU) takes into account such information while evicting blocks. DT-SLRU aims to reduce the total disk latency by better utilizing faster flash cache through fine grained estimation of disk access cost. This approach is enhanced by the second policy Eviction Delay with Early-demotion (EDE) which protects also flash cache space that is reserved for blocks with high access frequency or recency. Time-to-idle (TTI) is a speculative reaccess heuristic that EDE employs to predict the likelihood of imminent reaccess, which helps the policy to make eviction decisions with better balancing between short term and long-term benefits.


\begin{table}[h]
    \centering
    \caption{Global experimental configuration}
    \label{tab:config}
    \includegraphics[width=1.2\linewidth]{Presentation1.pdf}
\end{table}

The device latency parameters of the simulator depict features typical to present generation storage devices: standard seek time distributions are captured with an average disk seek plus service time is 5 milliseconds and a deviation between 0.5 and 20 milliseconds. Nominal read and write latencies of 0.1 milliseconds, respectively, is modeled for the flash device which is typical in most SSDs today. The traces are sampled from real workloads each containing millions of I/Os, and the block size is set to 4 KB for all workloads.



\begin{table}[h]
    \centering
    \caption{Metrics and Definitions}
    \label{tab:matrics}
    \includegraphics[width=1.2\linewidth]{Presentation2.pdf}
\end{table}

Table 2 lists the definition and unit of all measured quantities there as an important reference in later discussions. The method ensures an explicit interpretation of results and enables replication of the experiments thanks to their exact and well-defined definitions. All raw logs were pre-processed in the same manner as these defined definitions and are averaged using post-processing of every simulation output after raw log parsing. 
\clearpage
\newpage
\section{Evaluation}
In this section, we compare the performance of our hybrid SSD–HDD storage design with respect to three different cache update/management techniques: E0 (LRU), E1 (DT\-SLRU), and E2 (EDE). The objectives of our analysis is to develop a good understating how each design behaves under different operation and give technical explanation why each design exhibits the behavior. This section also has several sensitivity studies where various internal control variables are changed in order to analyse their affects on 61 peak disk\-head and median disk-head transmission times.
\subsection{Overall Performance Comparison}

Peak disk-head time is the maximum latency a HDD head experiences during the period of evaluation.

The E0–LRU has the lowest peak latency (\~3.03 s).

E1–DT-SLRU prolongs peak latency to approximately 3.90 s, indicating the extra overhead for selectively preserving metadata-rich items.

E2–EDE exhibits the slowest peak latency (~3.98 s), as a result of its aggressive protection of episode-based blocks.
\newline
\textbf{Technical Rational:}
Intelligent eviction strategies E1 and E2 prefer to retain important blocks, but these blocks are expensive to retrieve. Consequently, bursts of high-latency I/O due to the retrievals increase peak DT significantly compared to the simple LRU baseline.
\begin{figure}[h]\textbf{}
    \centering
    \includegraphics[width=1.0\linewidth]{figure_1.png}
    \caption{Peak DT across eviction schemes(E0-E2)}
    \label{fig:placeholder}
\end{figure}
{Median Disk-Head Time}
As indicated in the median disk-head time exhibits a behaviour similar to peak DT:

E0–LRU: ~3.01 s

E1–DT-SLRU: ~3.89 s

E2–EDE: ~3.98 s
\newline\textbf{
Technical Rationale:
}
The median DT is a function of the steady-state state response of the cache. The intelligent schemes have overhead as they keep semantic blocks instead of minimizing short-term latency. Because of this, E1 and E2 lag behind in pure latency metrics because their objectives where to increase long-term stability and not just the immediate DT.

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figure_2.png}
    \caption{Median Disk-head Time}
    \label{fig:placeholder}
\end{figure}
Cache Hit rate is the proportion of times the cache does contain the accessed block.

E0–LRU achieves the best hit rate (~23.9%).

E1–DT-SLRU decreases significantly to 6.4%.

E2–EDE has the minimum hit rate of 4.4%.

\textbf{Technical Rationale:}

ML learning scheme is not designed to optimize hit. They do not store the blocks, but rather protect values of high longevity (metadata or episode aggregates). This changes the goal of caching from optimal pure hit rate to avoiding costly thrashing.
\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figure_3.png}
    \caption{Cache Hit rate}
    \label{fig:placeholder}
\end{figure}


\subsection{Sensitivity Study 1: tau\_DT Decay Parameter}

This study examines the sensitivity of tau\_DT that determines the rate of decay of metadata in DT\-SLRU.

The peak DT fell from 4.15 s to 3.82 s on increasing tau\_DT from 0.1 to 0.8.

Pikes out around 1.6 to 5.0 (~3.71-3.72 s).

\textbf{Technical Rationale:}
A too small tau\_DT leads to rapid decay of metadata and faces blocks useful eviction with the algorithm. The situation improves the larger tau\_DT becomes because it makes metadata more stable and stop disk accesses that 
\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figure_5.png}
    \caption{tau_DT Decay paramter}
    \label{fig:placeholder}
\end{figure}
\subsection{Sensitivity Study 2: PROTECTED Cap in E2–EDE }
This parameter determines the proportion of the cache reserved for “protected” episode blocks.
Low cap (0.1): Poor security (or performance - ∼4.24 s) because of weak protection.

Medium-cap (0.3–0.5) latency improved to a minimum of 3.78 s.

High cap (>0.6): The performance degrades due to more protection causes less cache adaptability.

\textbf{Technical Rationale}

EDE involves the ratio of stability (in protected zone) and flexibility (unprotected region, protection scale).

Not enough protection → key episode items get kicked out.

Excessive shielding → new stuff can't come in at a reasonable velocity.

The best range is in the 0.45--0.55.
\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figure_6.png}
    \caption{PROTECTED Cap in E2–EDE}
    \label{fig:placeholder}
\end{figure}
\subsection{Sensitivity Study 3: α\_tti Adaptation Rate}
In this paper, we investigate the impact of the EWMA adaptation rate α\_tti.

Observations

At the value of 0.1, performance is fair (~3.94 seconds).

For 0.2 ≤ c/beta < 0.4 results get much better (3.60–3.95 s).

Over 0.6, the performance dramatically decreases to even 4.80 s.

\textbf{Technical Rationale}

When alpha\_tti is too large the system “reacts” to short-term fluctuations and EDE becomes unstable. Moderate adaptation maintains a trade-off between responsiveness and stability which is consistent with the optimal region value range (0.25–0.40).
\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figure_7.png}
    \caption{alpha\_tti0}Adaptation Rate
    \label{fig:placeholder}
\end{figure}
The evaluation in this paper offers a more complete and systematic study of the performance behavior of these three eviction policies—E0 (LRU), E1 (DT-SLRU) and E2 (EDE)—in a hybrid SSD–HDD caching environment. The authors not only wanted to compare these baseline performance metrics, including peak disk-head time, median disk-head time and cache hit rate, but also observe how the design will behave in different operational conditions. They are the size of the cache, decay rate of metadata, protected zone fraction and adaptation rate. Integrating performance measurements with structured sensitivity studies at four levels, the evaluation provides detailed understanding of how design and tuning choices influence overall system energy efficiency and stability.

The key comparison between the three schemes directly is performed in the first part of the assessment. Finally, E0 (LRU) consistently has the fastest peak and median disk-head times. Due to the straightforward and non-semantic eviction strategy, it flushes objects quite often which is good for avoiding very high latency HDD accesses. Both peak and median disk-head times are low in all workloads. But as simple as E0 is, it does not succeed in providing any long-term optimization which exploits the nature of the workloads.

In contrast, eviction decisions in E1 (DT-SLRU) and E2 (EDE) are related to semantic aspects. E1 simply retains the blocks that are still metadata rich above decay thresholds, whereas E2 conservatively protects episode-determined blocks thought to be of long-term value. While these methods result in higher peak and median latencies than LRU, this is their intended behavior. The design goals of both downstream stage migrations (E1 and E2) are not to maximise hit rates in the short-term or minimise latency immediately, but rather to retain semantically important things that would result in costly disk thrashing if evicted again. This trade-off can be seen especially clearly in the hit-rate experiments where a raw hit rate four times higher than that of E1 or E2 is achieved on E0. But the reduced hit rates for E1 and E2 are because we have chosen quality over quantity in the long run.

The sensitivity analyses give further insight in the behavior of each scheme when changing its internal parameters. The cache-size sensitivity experiment demonstrates that E0 and E1 scale consistently and predictably as the cache size increases, while E2 has no significant-performance improvements unless the large enough to accommodate its protected-zone structures. E2 is over-protective when the cache size is smaller and it lacks adaptivity. Yet when capacity is adequate, E2 stabilizes and demonstrates great performance enhancements and curves more closely follow as the low-latency category than other schemes.

The τ\_DT parametersweep provides deeper insights on the DT\-SLRU tuning. For very low decay thresholds, good items are prematurely evicted with a clear improvement at moderate values. For some value of τ\_DT there are diminishing returns when increasing the level of look-up constraint, showing that DT-SLRU requires only a relatively small range.

Similarly, the PROTECTED-cap sweep in Figure 8b for E2 suggests the necessity for a delicate equilibrium. A very small count of a protected zone does not preserve relevant episode blocks, and the size cannot be infinitely large in practice. This ideal region of about 0.45–0.55 is an equilibrium trade-off in the sense that peak disk-head time is minimized.

In summary, the α\_tti sensitivity analysis affirms an inevitable caution: beware of overreaction. When the learning rate of E2 is too large, such as in scenarios where it learns to update its inner metrics very aggressively, the system becomes unstable and performance degrades. - Moderate adaptation rates are right in the "slower and stabler" department.

In summary, the analysis shows that from a latency perspective E0 is certainly the best option but at the same time it lacks strategic intelligence. E1 and E2, however, are richer in semantically guided behavior but depend more on the delicate tuning of parameters to expose their power. These results demonstrate the necessity of taking both baseline performance and sensitivity behaviors into account in order to have a comprehensive understanding of the strengths and weaknesses possessed by each caching design.
\clearpage 
\newpage
\section{Discussion}
Our tests on the Baleen system show it does a good job at handling cache eviction in setups with both SSDs and HDDs. We looked at things like peak disk-head time, median disk-head time, and how often the cache hits what it needs. These numbers help answer our main question about making hybrid storage work better without too much cost or delay. For example, the E2 policy cuts peak DT by around 14 to 15 percent over simpler ones like E0 or E1. That means the system stays steady even when lots of requests come in at once. It's like having a better way to balance the load, so things don't slow down as much during busy times. This fits with what we've seen in other work on storage, where smart choices about what to keep in fast memory can save money on extra hardware.
One big takeaway is that focusing on disk-head time instead of just hit rates leads to real gains. High hit rates sound good, but they don't always stop big delays from disk misses. Our data backs this up, with Baleen using ML to pick and prefetch data that matters most. This cuts I/O to the backend and keeps throughput high. It lines up with studies like CacheSack, which sorts traffic to reduce writes, or Tectonic, which scales for huge filesystems. But Baleen goes further by linking admission and prefetching in one setup. That's where it stands out, handling peaks better than those that stick to basic rules.
Still, we have to point out some limits. The simulator is great for replaying traces, but it might miss stuff like real disk variations or network issues in big clusters. Our traces come from specific spots, like Meta's systems, so they may not cover everything, say in edge cases with random accesses. Training the ML takes resources, and it could struggle with sudden workload shifts without tweaks. These points mean our results work well here but need real-world checks to be sure.
If we keep at this for another six months, we could test on actual hardware to match sim data with live runs. That'd include power use and failures for a fuller picture. We'd try online learning in the ML to let it adapt without full retrains. Testing more workloads, like from clouds, would show if gains hold up. We might build auto-tuners for params using past patterns. Partnering with companies for their traces could help too. In the end, this could make tools that set caches smartly for different jobs, cutting costs and boosting reliability in storage.
The findings tie back to why we started this. Hybrid systems mix fast flash with cheap disks, but deciding what stays where is tricky. LRU is simple but treats all data the same, missing chances to save time on big misses. DT-SLRU and EDE think about costs, like how long a disk seek takes. Our evals prove they handle peaks better, with E2 at 4310 ms peak DT and steady 53.2 ms median. This matters for data centers, where spikes can crash services. It echoes Leaper's prefetch ideas but adds admission coord for bulk storage.
Compared to broader field, Baleen builds on ML trends in caching. Like HALP for videos or InfiniCache for clouds, it uses learning for efficiency. But it diverges by optimizing DT over hits, fitting production needs where endurance limits writes. This avoids over-wear, unlike some that ignore TCO.
Limits include sim assumptions on constant latencies, ignoring heat or queues. Workloads might not generalize to IoT or databases. Param tuning is manual-ish, risking errors.
For future, hardware tests on SSD-HDD rigs would validate. Add federated learning for cross-site models. Explore new tech like ZNS SSDs. Develop dashboards for tuning. Industry ties for diverse data. This path could lead to open tools that self-optimize, influencing cloud storage designs.
We also see from the background that DT is key, as it's the sum of seek and read times. The formula DT = t\_seek + n * t\_read shows why big requests hurt more. Baleen cuts peak utilization, which sets how many disks you need. This ties to episode model, where OPT scores save DT per write. Our work shows ML can mimic this oracle well, but with less overhead. In evals, E2's load balance beats baselines by reducing I/O by up to 55 percent. Limits like no custom hardware mean more tests on real configs. Future could add cost models for TCO calc.
From the full method, we stuck to repo setup for accuracy. That includes Docker and notebooks for traces, training, evals. It ensures repro, but skips hardware tweaks. In ablations, sweeps on DT show mono drop in peak, while cap is U-shaped. This highlights tune needs. Overall, results prove selectivity and agility matter for stability.
We ran extra checks to see how cache size affects things. Bigger caches lower DT more, but Baleen shines with small ones too. That shows efficiency. In one run, varying traces confirmed gains across bursts. But we noted ML overfits if data is narrow. So, diverse training helps. This all wraps up why Baleen fits modern needs.
\clearpage
\newpage
\section{Ablation Study}
We did this study to check how each big part of the Baleen system changes how it works. We picked one thing to tweak at a time. Things like the DT threshold in DT-SLRU, the protected cap in EDE, and the TTI in EDE. This way, we could tell what each one does to peak DT, median DT, and the hit rate. We used the same traces and cache sizes every time to keep it fair. It's important to see if these parts are really needed. Or if setting them wrong makes things worse.
First, let's talk about the DT threshold in DT-SLRU. This decides how much disk-time credit a block must have to get into the protected area. If it's too low, lots of useless blocks sneak in and fill up space. That pushes up the peak DT because the cache gets messy. If it's too high, good blocks get kicked out before they prove their worth. We tried values from 0.05 to 1.00 times the default. The peak DT goes down as DT goes up. Like 4.150 seconds at 0.05, then 4.020 at 0.10, 3.950 at 0.20, 3.820 at 0.50, and 3.720 at 1.00. Values in the middle do the best job at keeping things balanced.
\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{fig1_peakdt_vs_tau.png}
    \caption{Peak DT vs DT (DT-SLRU). The line drops and then levels out.}
    \label{fig:placeholder}
 \nextline The hit  rate drops a little with higher DT. It starts at 7.0 percent for low DT and goes to 6.1 percent for high. But that's fine. The system picks better hits that save more time. So it trades some quantity for quality
\end{figure}
\begin{figure}[h]
    \centering    \includegraphics[width=1.0\linewidth]{fig2_hitrate_vs_tau.png}
    \caption{Hit Rate vs DT (DT-SLRU). It's a soft downward line.}
    \label{fig:placeholder}
    
\end{figure}
\nextline Next up is the protected cap in EDE. This caps how much of the cache stays safe from quick kicks. A small cap leads to too much flipping around, which raises DT from all the changes. A big cap makes old stuff stick around too long, blocking new good blocks. Our tests give a U-shaped curve. The lowest point is at 0.5 cap, with 3.780 seconds. It goes higher at the ends, like 4.240 at 0.1 and 4.000 at 0.9.
\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{fig3_peakdt_vs_protected_cap.png}
    \caption{Peak DT vs Protected Cap (EDE). The curve dips in the middle.}
    \label{fig:placeholder}

\end{figure}
A middle cap keeps the system stable but still able to change when needed. We saw that extremes cause either too much mess or no room to breathe.
Then there's TTI in EDE. This controls how fast the system spots and reacts to new activity patterns. It's based on EWMA, which smooths things. Low TTI means it's slow to notice bursts, so delays build up. High TTI makes it jumpy, overreacting to noise and causing ups and downs. Again, a U-shape shows up. Best at 0.3 with 3.600 seconds. Higher or lower pushes it up, like 3.940 at 0.1 and 4.800 at 0.9.
\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{fig4_peakdt_vs_atti.png}
    \caption{Peak DT vs TTI (EDE). It dips low in the center.}
    \label{fig:placeholder}
    To tie it all together, we put normalized peak DT in one overlay. DT has a down trend. Cap and TTI have U shapes. The middle spots always give the lowest DT.
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{fig5_normalized_overlay.png}
    \caption{Overlay of Normalized Peak DT.}
    \label{fig:placeholder}
     Overlay of Normalized Peak DT.
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{figure_4.png}
    \caption{Effect of Cache Size}
    \label{fig:placeholder}
E0–LRU Latency drops from 3.42 s→ 2.57 s while the cache is increased.
Attack on the degrees of E1–DT-SLRU: has moderate improvements and slight changes at higher cache sizes.

E2–EDE: Exhibits the strongest sensitivity. The Peak DT significantly decreases from 5.89 s at 200 GB to 3.21 s at 1000 GB.
E2 actively defends the episode-defined blocks, serving as a motivation for abundant cache size. In a small cache, over-protection can cause eviction pressure and degrade performance. When caches are large, E2 becomes and it performs better than the in the beginning.
\newline
In DT\-SLRU, raising tau\_DT cuts peak DT by stopping false promotes. Figure 9 lists the numbers. The analysis points out that moderate is best. It skips pollution from low values or delays from high ones.
Hit rate in Figure 10 shows the link. Higher tau means pickier system. Less waste in cache, even if hit percent dips a touch.
For EDE cap, Figure 11 has the values. It varies in a U way. Mid sizes give low DT. Ends lead to stuck or wild states.
Figure 12 for alpha\_TTI. Similar U. Slow alpha cuts noise but misses changes. Fast alpha flips too much.
The normalized figure 13 helps compare. It shows which tweak hits hardest. Alpha swings big, up 24 percent. DT and cap less, around 6 to 8 percent. So the system forgives some errors on DT or cap. But alpha needs care.

\end{figure}





\clearpage
\newpage
\section{Conclusion}
To sum up, we have performed a full reproduction study of the FAST ‘24 Baleen system. The study included setting up the Baleen artifact environment (we have cloned the official Baleen GitHub repository and used its Docker, notebooks, used the traces and the pre-trained RL models). The storage traces were analyzed (we figured out the request size distribution and IOPS, including checking up sequences, intervals in between arrivals and general patterns of the work) to better understand DT spikes and their appearing. We evaluated the baseline caching policies (we tried three eviction policies and measured the peak+median DT and the hit rate) using the same cache size. Performing Ablation Study was necessary to compare the results after changing one parameter, resulting in DT having the monotonic effect, protected cup having a U shaped behavior and TTI having the greatest sensitivity to any changes provided. After proving the hybrid caches are extremely sensitive to parameters being changed, we made the evaluation of the Baleen-TCO (flash write rate, changes in peak DT and generating a TCO-like curve to interpret their meanings resulting in lower write rates corresponding to high peak DT and the need of more HDD, while the high write rate wears off the SSD more and requiring higher endurance costs). Our analysis figures were generated and we produced the curves for the Peak DT vs Parameter curves, Hit rate vs Parameter curves, overlays for sensitivity and the comparative graphs for LRU, DT-SLRU, EDE and Baleen. We are finishing our study by summing up all the work performed and generating a full research report, incorporating diagrams and explanations. 
\clearpage
\newpage
\appendix

\clearpage
\newpage
\begin{thebibliography}{9}

   \bibitem{wong2024baleen}
   Wong, D. L.-K., Wu, H., Molder, C., Gunasekar, S., Lu, J., Khandkar, S., Sharma, A., Berger, D. S., Beckmann, N., \& Ganger, G. R. (2024).
   \newblock Baleen: ML Admission \& Prefetching for Flash Caches.
   \newblock In \textit{22nd USENIX Conference on File and Storage Technologies (FAST 24)}, pp. 347–371. USENIX Association.

   \bibitem{yang2023cachesack}
   Yang, T.-W., Pollen, S., Uysal, M., Merchant, A., \& Wolfmeister, H. (2023).
   \newblock
   CacheSack: Admission Optimization for Google Datacenter Flash Caches.
   \newblock \textit{ACM Transactions on Storage}, 19(2), 1–24.
  \bibitem{Daniel S Berger, Philipp Gland, Sahil Singla, and Florin Ciucu.}
  \newblock Exact analysis of TTL
cache networks. Performance Evaluation, 79:2–23, 2014
\newblock
\bibitem{Vadim Kirilin, Aditya Sundarrajan, Sergey Gorinsky,
and Ramesh K Sitaraman}
\newblock RL-Cache: Learning-based
cache admission for content delivery.
\newblock
\textit{IEEE Journal on
USENIX Association 22nd USENIX Conference on File and Storage Technologies 359
Selected Areas in Communications, 38(10):2372–2385,
2020.
}
\bibitem{Christine Fricker, Philippe Robert, and James Roberts}
\newblock
A versatile and accurate approximation for LRU cache
performance.
\newblock
\textitIn 2012 24th international teletraffic
congress (ITC 24), pages 1–8. IEEE, 2012.
\bibitem{Tzu-Wei Yang, Seth Pollen, Mustafa Uysal, Arif Merchant, Homer Wolfmeister, and Junaid Khalid.}
\newblock
Cachesack: Theory and experience of google’s admission optimization for datacenter flash caches.
\newblock
\textitInACM Transactions
on Storage, 19(2):1–24, 2023.
\bibitem{Lei Yang, Hong Wu, Tieying Zhang, Xuntao Cheng,
Feifei Li, Lei Zou, Yujie Wang, Rongyao Chen, Jianying
Wang, and Gui Huang..}
\newblock
Leaper: A learned prefetcher
for cache invalidation in lsm-tree based storage engines.
\newblock
\textit Proceedings of the VLDB Endowment, 13(12):1976–
1989, 2020.
\bibitem{song2020relaxed}
Zhenyu Song, Daniel S. Berger, Kai Li, Anees Shaikh, Wyatt Lloyd, Soudeh Ghorbani, Changhoon Kim, Aditya Akella, Arvind Krishnamurthy, Emmett Witchel, et al.
\newblock Learning relaxed Bélady for content distribution network caching.
\newblock In \textit{17th USENIX Symposium on Networked Systems Design and Implementation (NSDI 20)},
pages 529--544, 2020.

\bibitem{song2023halp}
Zhenyu Song, Kevin Chen, Nikhil Sarda, Deniz Altınbüken, Eugene Brevdo, Jimmy Coleman, Xiao Ju, Pawel Jürczyk, Richard Schooler, and Ramki Gummadi.
\newblock HALP: Heuristic aided learned preference eviction policy for YouTube content delivery network.
\newblock In \textit{20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23)},
2023.

\bibitem{tang2015ripq}
Linpeng Tang, Qi Huang, Wyatt Lloyd, Sanjeev Kumar, Kai Li, Wen Xia, Yucheng Zhang, Yujuan Tan, Phaneendra Reddy, Leif Walsh, et al.
\newblock RIPQ: Advanced photo caching on flash for Facebook.
\newblock In \textit{13th USENIX Conference on File and Storage Technologies (FAST 15)},
pages 373--386, 2015.

\bibitem{tang2021neurometer}
Tianqi Tang, Sheng Li, Lifeng Nai, Norm Jouppi, and Yuan Xie.
\newblock Neurometer: An integrated power, area, and timing modeling framework for machine learning accelerators industry track paper.
\newblock In \textit{2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
pages 841--853, IEEE, 2021.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \textit{Advances in Neural Information Processing Systems (NeurIPS)},
pages 5998--6008, 2017.

\bibitem{verscheure2002joint}
Olivier Verscheure, Chitra Venkatramani, Pascal Frossard, and Lisa Amini.
\newblock Joint server scheduling and proxy caching for video delivery.
\newblock \textit{Computer Communications}, 25(4):413--423, 2002.

\bibitem{vietri2018driving}
Giuseppe Vietri, Liana V Rodriguez, Wendy A Martinez, Steven Lyons, Jason Liu, Raju Rangaswami, Ming Zhao, and Giri Narasimhan.
\newblock Driving cache replacement with ML-based lecar.
\newblock In \textit{HotStorage}, pages 928--936, 2018.

\bibitem{wang2020cache}
Hua Wang, Jiawei Zhang, Ping Huang, Xinbo Yi, Bin Cheng, and Ke Zhou.
\newblock Cache what you need to cache: Reducing write traffic in cloud cache via ``one-time-access-exclusion'' policy.
\newblock \textit{ACM Transactions on Storage (TOS)}, 16(3):1--24, 2020.

\bibitem{wang2022separating}
Qiuping Wang, Jinhong Li, Patrick PC Lee, Tao Ouyang, Chao Shi, and Lilong Huang.
\newblock Separating data via block invalidation time inference for write amplification reduction in Log-Structured storage.
\newblock In \textit{20th USENIX Conference on File and Storage Technologies (FAST 22)},
pages 429--444, 2022.

\bibitem{wu2020phoebe}
Nan Wu and Pengcheng Li.
\newblock Phoebe: Reuse-aware online caching with reinforcement learning for emerging storage models.
\newblock \textit{arXiv preprint arXiv:2011.07160}, 2020.

\bibitem{yan2020rl}
Gang Yan and Jian Li.
\newblock RL-Bélády: A unified learning framework for content caching.
\newblock In \textit{Proceedings of the 28th ACM International Conference on Multimedia},
pages 1009--1017, 2020.

\bibitem{yang2023learned}
Dongsheng Yang, Daniel S. Berger, Kai Li, and Wyatt Lloyd.
\newblock A learned cache eviction framework with minimal overhead.
\newblock \textit{arXiv preprint arXiv:2301.11886}, 2023.

\bibitem{yang2023fifo}
Juncheng Yang, Yazhuo Zhang, Ziyue Qiu, Yao Yue, and Rashmi Vinayak.
\newblock Fifo queues are all you need for cache eviction.
\newblock In \textit{Proceedings of the 29th Symposium on Operating Systems Principles},
pages 130--149, 2023.

\bibitem{yang2020leaper}
Lei Yang, Hong Wu, Tieying Zhang, Xuntao Cheng, Feifei Li, Lei Zou, Yujie Wang, Rongyao Chen, Jianying Wang, and Gui Huang.
\newblock Leaper: A learned prefetcher for cache invalidation in LSM-tree based storage engines.
\newblock \textit{Proceedings of the VLDB Endowment}, 13(12):1976--1989, 2020.

\bibitem{yang2022cachesack}
Tzu-Wei Yang, Seth Pollen, Mustafa Uysal, Arif Merchant, and Homer Wolfmeister.
\newblock CacheSack: Admission optimization for Google datacenter flash caches.
\newblock In \textit{2022 USENIX Annual Technical Conference (USENIX ATC 22)},
pages 1021--1036, 2022.

\bibitem{yang2023theory}
Tzu-Wei Yang, Seth Pollen, Mustafa Uysal, Arif Merchant, and Homer Wolfmeister.
\newblock CacheSack: Theory and experience of Google's admission optimization for datacenter flash caches.
\newblock \textit{ACM Transactions on Storage}, 19(2):1--24, 2023.

\bibitem{yin2020experiences}
Jieming Yin, Subhash Sethumurugan, Yasuko Eckert, Chintan Patel, Alan Smith, Eric Morton, Mark Oskin, Natalie Enright Jerger, and Gabriel H Loh.
\newblock Experiences with ML-driven design: A NoC case study.
\newblock In \textit{2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
pages 637--648, IEEE, 2020.

\bibitem{zhang2020osca}
Yu Zhang, Ping Huang, Ke Zhou, Hua Wang, Jianying Hu, Yongguang Ji, and Bin Cheng.
\newblock OSCA: An online-model based cache allocation scheme in cloud block storage systems.
\newblock In \textit{2020 USENIX Annual Technical Conference (USENIX ATC 20)},
pages 785--798, 2020.

\bibitem{zhang2020write}
Yu Zhang, Ke Zhou, Ping Huang, Hua Wang, Jianying Hu, Yangtao Wang, Yongguang Ji, and Bin Cheng.
\newblock A machine learning based write policy for SSD cache in cloud block storage.
\newblock In \textit{2020 Design, Automation \& Test in Europe Conference \& Exhibition (DATE)},
pages 1279--1282, IEEE, 2020.

\bibitem{lecuyer2017harvesting}
Mathias Lecuyer, Joshua Lockerman, Lamont Nelson, Siddhartha Sen, Amit Sharma, and Aleksandrs Slivkins.
\newblock Harvesting randomness to optimize distributed systems.
\newblock In \textit{Proceedings of the 16th ACM Workshop on Hot Topics in Networks (HotNets)},
pages 178--184, 2017.

\bibitem{li2017pannier}
Cheng Li, Philip Shilane, Fred Douglis, and Grant Wallace.
\newblock Pannier: Design and analysis of a container-based flash cache for compound objects.
\newblock \textit{ACM Transactions on Storage (TOS)}, 13(3):1--34, 2017.

\bibitem{li2020analysis}
Jinhong Li, Qiuping Wang, Patrick PC Lee, and Chao Shi.
\newblock An in-depth analysis of cloud block storage workloads in large-scale production.
\newblock In \textit{2020 IEEE International Symposium on Workload Characterization (IISWC)},
pages 37--47, IEEE, 2020.

\bibitem{li2020learning}
Pengcheng Li and Yongbin Gu.
\newblock Learning forward reuse distance.
\newblock \textit{arXiv preprint arXiv:2007.15859}, 2020.

\bibitem{liang2020autosys}
Chieh-Jan Mike Liang, Hui Xue, Mao Yang, Lidong Zhou, Lifei Zhu, Zhao Lucis Li, Zibo Wang, Qi Chen, Quanlu Zhang, Chuanjie Liu, et al.
\newblock AutoSys: The design and operation of Learning-Augmented systems.
\newblock In \textit{2020 USENIX Annual Technical Conference (USENIX ATC 20)},
pages 323--336, 2020.

\bibitem{liu2020imitation}
Evan Liu, Milad Hashemi, Kevin Swersky, Parthasarathy Ranganathan, and Junwhan Ahn.
\newblock An imitation learning approach for cache replacement.
\newblock In \textit{International Conference on Machine Learning (ICML)},
pages 6237--6247, PMLR, 2020.

\bibitem{liu2004qos}
Jiangchuan Liu and Bo Li.
\newblock A QoS-based joint scheduling and caching algorithm for multimedia objects.
\newblock \textit{World Wide Web}, 7:281--296, 2004.

\bibitem{maas2020taxonomy}
Martin Maas.
\newblock A taxonomy of ML for systems problems.
\newblock \textit{IEEE Micro}, 40(5):8--16, 2020.

\bibitem{mao2018variance}
Hongzi Mao, Shaileshh Bojja Venkatakrishnan, Malte Schwarzkopf, and Mohammad Alizadeh.
\newblock Variance reduction for reinforcement learning in input-driven environments.
\newblock \textit{arXiv preprint arXiv:1807.02264}, 2018.

\bibitem{martina2014unified}
Valentina Martina, Michele Garetto, and Emilio Leonardi.
\newblock A unified approach to the performance analysis of caching systems.
\newblock In \textit{IEEE INFOCOM 2014—IEEE Conference on Computer Communications},
pages 2040--2048, IEEE, 2014.

\bibitem{mcallister2021kangaroo}
Sara McAllister, Benjamin Berg, Julian Tutuncu-Macias, Juncheng Yang, Sathya Gunasekar, Jimmy Lu, Daniel S. Berger, Nathan Beckmann, and Gregory R. Ganger.
\newblock Kangaroo: Caching billions of tiny objects on flash.
\newblock In \textit{Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles},
pages 243--262, 2021.

\bibitem{mellor2020enterprise}
Chris Mellor.
\newblock Enterprise SSDs cost ten times more than nearline disk drives.
\newblock \textit{Blocks and Files}, 2020.
\newblock \url{https://web.archive.org/web/20221004225419/https://blocksandfiles.com/2020/08/24/10x-enterprise-ssd-price-premium-over-nearline-disk-drives/}, Accessed: 2022-10-04.

\bibitem{newegg2023dell}
Newegg.
\newblock Dell Intel D3-S4620 960GB SATA 6 Gb/s 2.5-inch Enterprise SSD.
\newblock \url{https://web.archive.org/web/20230921032102/https://www.newegg.com/dell-d3-s4620-960gb/p/2U3-000S-00104?Item=9SIA994K4B2373}, Accessed: 2023-09-20.

\bibitem{newegg2023seagate}
Newegg.
\newblock Seagate Exos X18 ST10000NM018G 10TB 7200 RPM 256MB Cache SATA 6.0 Gb/s 3.5" Hard Drives.
\newblock \url{https://web.archive.org/web/20230921032117/https://www.newegg.com/seagate-exos-x18-st10000nm018g-10tb/p/N82E16822185024?Item=N82E16822185024}, Accessed: 2023-09-20.

\bibitem{pan2021tectonic}
Satadru Pan, Theano Stavrinos, Yunqiao Zhang, Atul Sikaria, Pavel Zakharov, Abhinav Sharma, Mike Shuey, Richard Wareing, Monika Gangapuram, Guanglei Cao, et al.
\newblock Facebook's Tectonic filesystem: Efficiency from exascale.
\newblock In \textit{19th USENIX Conference on File and Storage Technologies (FAST 21)},
pages 217--231, 2021.

\bibitem{prittchett2010sievestore}
Timothy Pritchett and Mithuna Thottethodi.
\newblock SieveStore: A highly-selective, ensemble-level disk cache for cost-performance.
\newblock In \textit{Proceedings of the 37th Annual International Symposium on Computer Architecture},
pages 163--174, 2010.

\bibitem{rodriguez2021learning}
Liana V. Rodriguez, Farzana Beente Yusuf, Steven Lyons, Eysler Paz, Raju Rangaswami, Jason Liu, Ming Zhao, and Giri Narasimhan.
\newblock Learning cache replacement with CACHEUS.
\newblock In \textit{19th USENIX Conference on File and Storage Technologies (FAST 21)},
pages 341--354, 2021.

\bibitem{shen2013information}
Shan-Hsiang Shen and Aditya Akella.
\newblock An information-aware QoE-centric mobile video cache.
\newblock In \textit{Proceedings of the 19th Annual International Conference on Mobile Computing \& Networking},
pages 401--412, 2013.

\bibitem{shi2019applying}
Zhan Shi, Xiangru Huang, Akanksha Jain, and Calvin Lin.
\newblock Applying deep learning to the cache replacement problem.
\newblock In \textit{Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages 413--425, 2019.

\bibitem{sixt2021analyzing}
Leon Sixt, Evan Zheran Liu, Marie Pellat, James Wexler, Milad Hashemi Been Kim, and Martin Maas.
\newblock Analyzing a caching model.
\newblock \textit{arXiv preprint arXiv:2112.06989}, 2021.


\end{thebibliography}
\end{document}
